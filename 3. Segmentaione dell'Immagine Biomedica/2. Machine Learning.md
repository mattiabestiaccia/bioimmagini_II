---
tags:
  - machine-learning
  - supervised-learning
  - unsupervised-learning
  - deep-learning
  - neural-networks
  - clustering
  - training-set
aliases:
  - Machine Learning
  - Apprendimento Automatico
  - Deep Learning
---

> [!nav]- Navigazione
> **Precedente:** [[1. Intro|‚Üê Introduzione Segmentazione]] | [[../INDICE|üìë Indice]]

## Machine Learning per la Segmentazione

Si parler√† di unsupervised learning se nel processo non viene utilizzato un set di dati validato, per cui lo scopo del processo √® trovare un modello di tipo probabilistico che descriva la configurazione dei dati. Vale la pena notare che anche negli algoritmi unsupervised si adotta un ‚Äúmodello‚Äù dei dati ottenuto dalla conoscenza del problema, anche se i dati non sono definiti in modo esplicito. Come semplice esempio, sia data una funzione f(x). Conosciamo alcuni campioni di f(x), cio√® una serie di coppie (x,f(x)). Vogliamo trovare una funzione h che approssimi al meglio possibile f sulla base dei campioni posseduti. In modo formale, ogni possibile funzione h √® una ipotesi di come sia f. Il problema √® verificare quanto l‚Äôipotesi h sia corretta e cambiarla in modo da ottenere delle h sempre pi√π simili a f. Supponiamo ora che f(x) sia una funzione di una variabile. Allora le coppie (x,f(x)) sono coppie di numeri reali. Limitiamo lo spazio di definizione di h (spazio delle ipotesi H) a tutti i polinomi di grado massimo k. Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini 3 4 5 6 7 8 9 10 11 0 1 2 3 4 5 6 7 8 f(x) X 2 4 6 8 10 12 14 0 1 2 3 4 5 6 7 8 f(x) X a b La figura a mostra una serie di tre campioni dove una funzione h lineare ottiene un perfetto fitting. Un perfetto fitting viene anche ottenuto da una funzione polinomiale di ordine superiore nella figura b. Un primo problema √® quale delle due ipotesi h dobbiamo preferire. Intuitivamente, possiamo supporre di scegliere l‚Äôipotesi pi√π semplice. Questo modo di procedere √® detto del rasoio di Occam: preferire l‚Äôipotesi pi√π semplice tra quelle consistenti con i dati. In generale definire il concetto di semplicit√† √® non banale, ma in questo caso particolare si pu√≤ considerare equivalente al grado del polinomio. E‚Äô evidente come la possibilit√† di trovare un‚Äôipotesi h che fitti correttamente i dati dipende fortemente dallo spazio delle ipotesi H. Se ad esempio f(x) √® una funzione sinusoidale, sar√† improbabile trovare una funzione polinomiale che la descriva correttamente. Invece utilizzando lo spazio H delle funzioni sinusoidali si otterrebbe facilmente un risultato corretto. In definitiva l‚Äôalgoritmo dipender√† da una ipotesi a priori sulla natura di f(x). In modo analogo, in un algoritmo di clustering il risultato dipender√† dal numero di cluster, che deriva da una ipotesi a priori sulla distribuzione dei dati. Molti algoritmi di segmentazione, come gli algoritmi di clustering, rientrano nella classe del unsupervised learning.
## Supervised Learning
Nel supervised learning, si suppone di avere un sistema con una certa funzione di trasferimento H() che mappa un certo input in un certo output. Ad esempio, l‚Äôinput potrebbe essere una immagine biomedica e l‚Äôoutput la maschera risultante da un processo di segmentazione.
Partendo da un esempio di coppia Input-Output IT-OT, detto Training set o insieme di addestramento, si vuole determinare H() tale che H(IT)-OT sia pi√π piccolo possibile. Inoltre si vuole che dato un nuovo insieme I1-O1, H(I1)-O1 sia ancora piccolo. Si tratta quindi di ‚Äúimparare‚Äù da un esempio (il training set) una regola generale che funzioni anche con set di dati simili.
![[Pasted image 20251129201800.png]]
A seconda della natura dei dati di output si possono distinguere tre casi. Nel classification learning i dati di output non hanno nessuna struttura particolare e l‚Äôunica cosa che si pu√≤ determinare e se due elementi dell‚Äôinsieme dei dati di output siano uguali o meno. In questo caso gli elementi dello spazio di output si chiamano classi. Un esempio di classification learning √® quello del riconoscimento dei caratteri scritti a mano. In questo caso le classi sono tutti i possibili caratteri e la classificazione di un carattere consiste nell‚Äôassegnarlo ad una certa classe. Nel preference learning tra due elementi non uguali √® possibile determinare quale sia il migliore e quale il peggiore, ma non di graduare la differenza tra i due elementi. Gli elementi dello spazio di output in questo caso sono chiamati ranks (ranghi). Esiste quindi un ordinamento tra gli elementi ma non il concetto di distanza metrica tra di essi. Nel function learning lo spazio di output √® uno spazio metrico, e quindi ogni elemento √® assegnato un valore di una funzione che √® continua. In questo caso, come si vedr√† esaminando le reti neurali, esiste un algoritmo di back-propagation che garantisce il raggiungimento di un massimo locale.
## Unsupervised Learning
## Misura delle Prestazioni
Per verificare la qualit√† del processo di apprendimento il processo stesso viene confrontato con un ‚Äúgold standard‚Äù, cio√® con un insieme di dati di test (test set) in cui f(x) √® nota. Il metodo √® schematizzabile come segue: ‚Ä¢ Ottenere un insieme il pi√π numeroso possibile di esempi ‚Ä¢ Dividere l‚Äôinsieme in due parti: training set e test set. ‚Ä¢ Applicare l‚Äôalgoritmo di learning al training test (test di addestramento) ottenendo H ‚Ä¢ Applicare H al test set e verificare l‚Äôerrore commesso ‚Ä¢ Ripetere i passi successivi per diverse scelte random del training test variando anche la dimensione dello stesso. Si ottiene una funzione della capacit√† dell‚Äôalgoritmo di trovare una ipotesi H consistente con i dati in funzione della grandezza del training test. Questa funzione rappresenta la cosiddetta learning curve che descrive la capacit√† dell‚Äôalgoritmo di apprendere da un insieme di dati di addestramento di una certa dimensione. Un esempio di learning curve √® in figura. Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini Una buona learning curve dovrebbe essere monotona crescente (happy graph). Le reti neurali convolutive (CNN) utilizzano in generale un approccio supervised learning. 
## Procedure di Ottimizzazione
Prima di introdurre in modo formale il concetto di segmentazione di immagini, √® opportuno definire il concetto di ottimizzazione, cio√® della ricerca della combinazione di parametri che massimizza (o minimizza) una certa funzione. Utilizziamo come esempio il cosiddetto problema TSP o Problema del Commesso Viaggiatore (Travelling Salesman Problem, da cui la sigla TSP). La definizione del problema √® abbastanza semplice: un commesso viaggiatore deve visitare un certo numero di clienti prima di tornare a casa. Conosce la posizione dei clienti e il tempo necessario a spostarsi dall'uno all'altro. Vuole ovviamente visitare tutti i clienti una sola volta nel tempo pi√π breve possibile. In termini pi√π formali, il problema consiste nel costruire un grafo i cui nodi rappresentano i clienti, mentre gli archi rappresentano i percorsi fra i nodi, e di trovare su di esso un ciclo che tocchi tutti i nodi una ed una sola volta e abbia la durata complessiva minima. Il problema, semplice da descrivere, √® per√≤ complesso da risolvere. Il numero delle sue soluzioni, infatti, cresce molto rapidamente con il numero dei nodi.
[...]
In questo caso abbiamo risolto il problema calcolando tutte le possibili soluzioni e scegliendo la soluzione ottima. Questo approccio √® detto a ricerca esaustiva, o brute force. Se il dominio di input √® finito, tali algoritmi trovano sempre la soluzione corretta. In generale, per n nodi il numero di soluzioni possibili sar√† (n-1)!, che cresce molto rapidamente con n. Per comprendere la difficolt√† di gestione di un problema di tale complessit√†, consideriamo un calcolatore capace di compiere 4 109 operazioni al secondo (circa 4 Gflops, dell‚Äôordine della potenza di calcolo di un PC a 4 GHz di uso comune). Ammettiamo di riuscire a computare la lunghezza di un percorso con n operazioni (in realt√† ce ne vorranno certamente di pi√π, bisognerebbe contare anche gli accessi in memoria in lettura o scrittura). Allora il computo di tutte le soluzioni richieder√† un numero di operazioni pari a:
NO = n*(n-1)! = n!
Per n=20 abbiamo N! = 2.4 1018 operazioni. Il tempo necessario con il calcolatore ipotizzato prima sar√† (consideriamo 3 107 sec in un anno): circa 20 anni.
L‚Äôesempio dimostra come al crescere della dimensione del problema non sia possibile risolvere lo stesso in modo esaustivo. Il problema TSP e un esempio della famiglia di problemi NP-completi, cio√® problemi di complessit√† che cresce in modo non lineare con la dimensione del problema. Tali problemi non possono essere risolti in modo esaustivo quando le dimensioni dei dati di input crescono sopra una certa dimensione. Anche se esistono approcci che consentono in alcuni casi di risolvere in modo esatto un particolare problema NP-completo, in generale quello che si pu√≤ fare √® trovare degli algoritmi che ottengano delle soluzioni approssimate con una complessit√† accettabile. Tali algoritmi non potranno comunque fornire mai una soluzione sicuramente ottima, proprio perch√© le soluzioni possibili non sono note e quindi √® impossibile determinare se una soluzione sia la migliore o meno. L‚Äôesempio pi√π semplice di soluzione non esaustiva √® la ricerca casuale o random. Invece di fare una ricerca esaustiva in modo sistematico utilizzando sempre lo stesso ordine di ricerca, possiamo scegliere un percorso di ricerca variabile in modo random volta per volta. Se viene esplorato tutto il dominino di input, tali algoritmi sono un caso particolare di un algoritmo esaustivo. Se viene esplorata solo una parte del dominio di input, gli algoritmi random hanno una certa probabilit√† di trovare la soluzione ottima pari al rapporto tra numero di percorsi esplorati e numero totale di percorsi. L‚Äôalgoritmo di ricerca casuale non ha utilit√† pratica, ma serve come confronto per gli altri algoritmi di ottimizzazione, nel senso che qualsiasi algoritmo di ottimizzazione ragionevole deve funzionare meglio della ricerca casuale. Un esempio di soluzione approssimata del problema TSP si pu√≤ ottenere con un approccio di tipo Greedy. Un esempio di programmazione Greedy sono gli algoritmi del tipo best-first-search, dove ci si muove in un grafo che descrive un problema scegliendo via via i nodi che sembrano migliori per risolvere il problema. Riconsideriamo il problema TSP visto in precedenza: Partendo dal nodo 1, il nodo alla minore distanza √® il nodo 2 (distanza 8). Ci muoviamo quindi nel nodo 2. Dal nodo 2 il nodo pi√π vicino √® il nodo 3 (distanza 3). Dal nodo 3 andremo nel 4 (distanza 6) e dal nodo 4 dovremo andare nel 5 (distanza 9) e poi nell‚Äô1 (distanza 10). Il percorso (1-2-3-4-5-1) ha lunghezza totale 8+3+6+9+10=36, che come si era visto √® la distanza minima. Con un algoritmo semplice abbiamo quindi trovato la soluzione ottima. In particolare il numero di passi dell‚Äôalgoritmo Greedy √® (N-1)+(N-2)+(N-3)+‚Ä¶..+1, infatti al primo passo devo fare N-1 confronti, al secondo N-2, etc. L‚Äôordine di grandezza del numero di passi √® N2 molto minore del numero di passi dell‚Äôalgoritmo esaustivo. In generale non √® detto che in questo modo si trovi la soluzione migliore, consideriamo ad esempio il grafo: Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini L‚Äôalgoritmo best-first-search ritrova il percorso ottimo visto prima 1-2-3-4-5-1=36, ma esiste un percorso migliore 1-5-2-3-4-1=35 che non viene ‚Äúvisto‚Äù. Questo conferma quanto si era detto a proposito del problema TSP, cio√® che esistono algoritmi di limitata complessit√† computazionale che danno una soluzione approssimata, ma non necessariamente la migliore. La soluzione prodotta dall‚Äôalgoritmo best-first-search dipende dal nodo iniziale che si sceglie per la partenza dell‚Äôalgoritmo. La figura mostra i risultati di un algoritmo best-first-search applicato al problema TSP Berlin52 variando il nodo di partenza dell‚Äôalgoritmo.
![[Pasted image 20251129202126.png]]


Si osserva come la soluzione dipenda dal nodo, la soluzione migliore √® 8182. Per confronto la migliore soluzione nota del problema Berlin52 √® 7542 Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini Per concludere, dato un certo problema NP-completo da risolvere per via algoritmica, esister√† una soluzione esaustiva che risolve il problema in maniere ottima, ma sar√† applicabile solo per dimensioni del problema molto piccole e non interessanti da un punto di vista pratico. Per risolvere il problema sar√† necessario utilizzare un algoritmo di ottimizzazione pi√π veloce dell‚Äôalgoritmo esaustivo. Tale algoritmo non potr√† in generale assicurare la soluzione ottima, ma solo una soluzione ‚Äúragionevolmente‚Äù buona. In generale la soluzione trovata da un algoritmo non esaustivo dipender√† dalle condizioni iniziali, e quindi varier√† in funzione delle condizioni iniziali stesse. In generale un processo di ottimizzazione sar√† definito da tre caratteristiche: ‚Ä¢ Spazio di ricerca (Search-space). Lo spazio di ricerca √® l‚Äôinsieme dei valori che possono assumere le possibili soluzioni. Ad esempio, nel problema TSP tutte le possibili liste dei nodi senza ripetizioni. ‚Ä¢ Metrica: La quantit√† da massimizzare o minimizzare, nel caso del TSP la lunghezza di un percorso. ‚Ä¢ Il processo di ottimizzazione: L‚Äôalgoritmo utilizzato per trovare all‚Äôinterno del search-space la soluzione che massimizza o minimizza la metrica. Nel caso del TSP il metodo greedy. Gli algoritmi di ottimizzazione possono essere classificati come: ‚Ä¢ Ottimizzatori locali. Un ottimizzatore locale trova una soluzione partendo da un punto del search-space (condizioni iniziali). La soluzione trovata dipender√† quindi dalle condizioni iniziali. Nel problema TSP, l‚Äôalgoritmo greedy applicato ad un singolo nodo iniziale √® un ottimizzatore locale e la soluzione dipende dal nodo selezionato. ‚Ä¢ Ottimizzatori globali. Un ottimizzatore globale trova la soluzione migliore all‚Äôinterno del search-space indipendentemente dai parametri di ingresso. Come detto in precedenza, l‚Äôunico vero ottimizzatore globale √® la ricerca esaustiva. Nel caso del TSP un algoritmo greedy iterato su tutti i nodi si pu√≤ considerare una approssimazione di un ottimizzatore globale, in quanto il risultato non dipende dalle condizioni iniziali.

## Segmentazione a Soglia
Nella segmentazione a soglia ogni pixel viene associato ad un tipo di tessuto mediante l‚Äôintensit√† di segnale. La segmentazione a soglia corrisponde quindi a scegliere una o pi√π soglie nell‚Äôistogramma dell‚Äôimmagine e a classificare i pixel mediante tale informazione. Consideriamo il fantoccio MR in figura:
![[Pasted image 20251129202220.png]]
I sei pattern sono associati a quattro diversi ‚Äútessuti‚Äù (aria, vetro, olio, acqua). L‚Äôaria e il vetro in MR sono due oggetti del tutto equivalenti, in quanto non forniscono alcun segnale MR. Sono quindi impossibili da distinguere con una segmentazione a soglia e li possiamo raggruppare in una classe ‚Äúsfondo‚Äù (background), ottenendo quindi tre classi. Analogamente le due regioni contenenti acqua hanno lo stesso segnale. Consideriamo l'istogramma dell'immagine. Appaiono i tre picchi che descrivono la distribuzione del segnale, con ampiezza proporzionale al numero di pixel. Per dividere le classi occorrono due soglie (il numero di soglie √® N-1 dove N √® il numero di classi). Fissiamo le soglie a T1=90 (sfondo-acqua) e T2=500 (acqua-olio). Estraiamo dall'immagine i pixel con valori tali che 
![[Pasted image 20251129202348.png]]
e poniamo ad un valore diverso da 0 tali pixel e a zero gli altri. Otteniamo quindi tre maschere (mask) che descrivono la distribuzione delle tre classi sull'immagine. Le tre maschere rappresentano (colore bianco) la distribuzione dei pixel appartenenti alle classi sfondo, acqua e olio rispettivamente. Notiamo che alcuni pixel sono stati erroneamente assegnati alla maschera ‚Äúsfondo‚Äù anche se sono acqua. Questo √® dovuto alla imperfetta separazione dei picchi sfondo-acqua nell'istogramma. Dall'esempio individuiamo i limiti fondamentali della segmentazione a soglia: 1) La segmentazione a soglia non √® in grado di distinguere oggetti topologicamente diversi ma ai quali √® associato lo stesso livello di segnale. Nel nostro caso i pattern 1-3-6 e 2-6 vengono rappresentati in una sola maschera. Questo rappresenta un problema in molte applicazioni cliniche (es: ventricolo destro e sinistro nel cuore, grasso viscerale e grasso subcutaneo, etc). 2) Possono apparire pixel spuri dovuti alle variazioni di segnale indotte dal rumore 3) Le soglie vengono definite manualmente tramite esame visivo dell'istogramma. Il punto 1) √® un limite intrinseco della segmentazione a soglia. Come si vedr√† nel seguito pu√≤ essere superato facendo seguire alla segmentazione a soglia l‚Äôapplicazione di algoritmi di tipo topologico applicati alle maschere prodotte dalla segmentazione a soglia, come il labeling. Il punto 2) implica che la qualit√† della segmentazione ottenibile con un algoritmo a soglia dipende sostanzialmente dal CNR tra i due tessuti da separare. Anche in questo caso gli errori di segmentazione possono essere corretti, almeno in parte, applicando algoritmi di filtraggio al risultato della segmentazione.
Per quanto riguarda il punto 3) esistono vari approcci automatici che consentono di trovare la soglia ‚Äúottima‚Äù che divide i pixel in classi attraverso soglie opportune. Tali algoritmi adottano un approccio ‚Äúunsupervised learning‚Äù.
## Algoritmo di Otsu
L'algoritmo di Otsu nella sua versione originale presume che nell'immagine da segmentare siano presenti due sole classi e quindi calcola la soglia ottima per separare queste due classi minimizzando la varianza intra classe. L'algoritmo pu√≤ essere esteso a pi√π classi (multi Otsu method). L'algoritmo di Otsu standard minimizza la quantit√† (varianza intra classe):

dove œâ √® la probabilit√† di una classe separata dall‚Äôaltra dalla soglia t e œÉ √® la SD della classe. Quindi l‚Äôalgoritmo cerca di trovare la soglia che separa l‚Äôimmagine in due sottoclassi che siano il pi√π possibile omogenee al loro interno. Intuitivamente, immaginando un istogramma a due picchi il metodo di Otzu consiste nel trovare la soglia t intermedia tra i due picchi che li divida in modo ottimale. Si dimostra che minimizzare la varianza intra-classe √® equivalente a massimizzare la varianza interclasse:

dove Œº √® il valor medio di una classe. Il metodo consiste nel provare in modo esaustivo tutti i possibili t (che sono in numero uguale alla profondit√† dell'immagine) e prendere il t che minimizza la varianza inter classe. Da un punto di vista algoritmico: 1) Si calcola l'istogramma h dell'immagine. La probabilit√† œâ si ottiene normalizzando l'istogramma per il numero di pixel dell'immagine (da un punto di vista pratico la normalizzazione non √® necessaria). 2) Si calcola il valore di œÉ2 b(t) per ogni t calcolando
![[Pasted image 20251129202445.png]]
3) Si sceglie il t che massimizza (œÉ_b)^2(t)
In questo modo la soglia t viene definita in modo automatico. In MATLAB il metodo di Otsu √® implementato nella funzione otsuthresh quando si utilizza l‚Äôistogramma estratto dall‚Äôimmagine o graythresh quando si opera direttamente sull‚Äôimmagine stessa. Il metodo di Otsu opera secondo un processo di ottimizzazione basato sulla ricerca esaustiva su tutti i possibili valori del parametro t da ottimizzare, ed √® quindi non particolarmente efficiente da un punto di vista computazionale. Tuttavia, essendo il calcolo della quantit√† œÉ2 b(t) da ottimizzare molto rapido il calcolo di t pu√≤ essere effettuato in tempi ragionevoli. Notiamo che il tempo di calcolo √® fortemente dipendente dal valore di profondit√† dell‚Äôimmagine che determina il numero di prove da effettuare e la lunghezza delle somme da calcolare. Si noti infine che possono esistere pi√π valori di t per cui œÉ2 b(t) √® massima, si pensi al caso di un istogramma con due picchi ben separati. In questo caso l‚Äôalgoritmo ritorner√† il valor medio tra i valori di t che massimizzano (œÉ_b)^2(t)
## Clustering
Un altro metodo per la scelta dei valori di soglia corretti che minimizzino l‚Äôerrore nella segmentazione √® l‚Äôuso di algoritmi di clustering. Il Clustering √® una tecnica di analisi dei dati volta alla selezione e raggruppamento di elementi omogenei in un insieme di dati. L‚Äôapproccio √® del tipo unsupervised, nel senso che l‚Äôalgoritmo di clustering riesce a dividere i dati in una serie di insiemi avendo come unico input il numero di insiemi da trovare. Gli algoritmi di Clustering si possono applicare a dati di diversa natura, e trovano applicazione in molteplici discipline come la bioinformatica, il marketing, la genetica, etc.
In generale i dati di ingresso di un algoritmo di clustering sono composti da una tabella con N righe, che corrispondono agli elementi da analizzare, e da K colonne che corrispondono alle osservazioni disponibili sugli elementi stessi. Ad esempio le righe della tabella potrebbero rappresentare dei pazienti e le colonne dei dati clinici sui pazienti (analisi del sangue, valori di pressione, etc). Lo scopo del clustering √® raggruppare gli elementi simili tra loro in gruppi (cluster) sulla base delle osservazioni. In figura √® esemplificato un problema classico di clustering: abbiamo un insieme di dati (caratterizzati da una coppia di valori) intuitivamente raggruppabili in quattro classi e vogliamo ottenere la soluzione a destra in cui i dati sono opportunamente raggruppati. In questo caso l‚Äôosservazione intuitiva che i dati si raggruppano in quattro cluster corrisponde all‚Äôutilizzo come distanza tra i dati della distanza geometrica sul piano. Punti tra loro ‚Äúvicini‚Äù secondo la distanza scelta sono assegnati allo stesso insieme.
In figura √® esemplificato un problema classico di clustering: abbiamo un insieme di dati (caratterizzati da una coppia di valori) intuitivamente raggruppabili in quattro classi e vogliamo ottenere la soluzione a destra in cui i dati sono opportunamente raggruppati. In questo caso l‚Äôosservazione intuitiva che i dati si raggruppano in quattro cluster corrisponde all‚Äôutilizzo come distanza tra i dati della distanza geometrica sul piano. Punti tra loro ‚Äúvicini‚Äù secondo la distanza scelta sono assegnati allo stesso insieme. Nel campo dell‚Äôanalisi delle immagini biomediche, le righe della tabella rappresentano le locazioni spaziali su cui vengono acquisite le immagini (quindi i pixel/voxel dell‚Äôimmagine), mentre le colonne rappresentano i valori di segnale acquisiti in corrispondenza delle locazioni spaziali. In definitiva, l‚Äôinput dell‚Äôalgoritmo di clustering sar√† una matrice NxM, dove N √® il numero di locazioni spaziali considerate (numero di pixel o voxel) e M il numero di acquisizioni disponibili. Quindi per una immagine 2D che contiene Nx x Ny pixel, la matrice di clustering corrispondente sar√† un matrice a Nx x Ny righe ed una singola colonna. Per una immagine 3D la matrice di clustering corrispondente sar√† una matrice a Nx x Ny xNz righe ed una singola colonna. Una immagine a colori RGB corrisponder√† ad una matrice a tre colonne (K=3), in quanto per ogni pixel sono disponibili tre misure corrispondenti alla relativa tripletta RGB.
Una immagine 2D +T corrisponder√† ad una matrice di clustering con N = Nx x Ny e K = nT, in quanto per ogni pixel sono disponibili nT osservazioni ad intervallo di tempo diversi. Analogamente una immagine 3D + nT corrisponder√† a una matrice di clustering con N = Nx x Ny x Nz e K = nT. E‚Äô importante notare che la corrispondenza ha senso se gli nT frame temporali sono allineati, cio√® se i pixel nella serie temporale corrispondono alle stesse locazioni spaziali. Ad esempio una serie temporale in cui il paziente √® fermo e viene iniettato un contrasto che cambia i livelli di grigio pu√≤ essere interpretata in questo modo, mentre una serie temporale che segue il battito cardiaco no. Altri esempi in MRI sono immagini T1, T2, PD pesate dello stesso distretto anatomico. Oppure potremmo avere immagini multiecho acquisite a tempi di eco diversi. Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini Gli algoritmi di clustering usati nella segmentazione di immagini si possono classificare in tre classi: 1) Clustering esclusivo (k-means) 2) Clustering non esclusivo (Fuzzy C-mean) 3) Clustering probabilistico Nel clustering esclusivo, un dato deve appartenere ad uno ed un solo cluster. L‚Äôimplementazione pi√π nota di questo approccio √® l‚Äôalgoritmo K-MEANS.
![[Pasted image 20251129202811.png]]![[Pasted image 20251129202820.png]]

## Algoritmo K-means
L‚Äôalgoritmo K-means (MacQueen, 1967) √® l‚Äôalgoritmo base ed uno dei primi ad essere stato realizzato per la classificazione di dati. Dato un insieme Y = (y1,‚Ä¶yn) di dati da classificare in C gruppi (cluster) e una distanza || || definita su Y, possiamo definire una funzione obiettivo da minimizzare. Questa funzione √®:
![[Pasted image 20251129202841.png]]
dove il contenuto della sommatoria √® √® la distanza tra un dato yj e un centroide vk. Per ogni dato vengono contate solo le distanze rispetto al centroide pi√π vicino. Questa funzione obiettivo √® un indicatore della distanza dei dati y dai centri dei rispettivi cluster. La distanza utilizzata pu√≤ essere una qualsiasi metrica. Il problema di trovare l‚Äôassegnazione dei dati ai cluster che realizzi il minimo di J √® NP-completo con complessit√† computazionale O(ndC+1 log n) dove d √® la dimensionalit√† dei dati. Il problema per n non piccolissimo non √® quindi risolubile in modo esaustivo e si utilizzano algoritmi in grado di trovare un minimo locale di J. La procedura pi√π nota √® abbastanza semplice e richiede la definizione di un numero di cluster (C) in cui dividere i dati. Si definiscono C centroidi, uno per ogni cluster da trovare, in modo casuale ma in modo che siano abbastanza lontani tra loro, in modo da migliorare la convergenza dell‚Äôalgoritmo. A questo punto per ogni dato calcoliamo la distanza dai centroidi ed associamo il dato al centroide pi√π vicino. Calcoliamo ora C nuovi centroidi, come centro di massa dei cluster ottenuti al passo precedente. Possiamo ora ricomputare la distanza di tutti i dati dai nuovi centroidi e creare cos√¨ dei nuovi cluster. Iterando il procedimento, arriveremo ad un punto in cui i centroidi si stabilizzano senza cambiare pi√π di posizione. Saremo quindi in una situazione di convergenza dell‚Äôalgoritmo che ci d√† il clustering desiderato. La procedura per trovare il valore minimo di J √® la seguente: ‚Ä¢ Assegnare un valore iniziale ai C centroidi vk ‚Ä¢ Iterare i due passi seguenti fino a quando i valori di vk si modificano: ‚Ä¢ Assegnare ogni dato y ad uno ed un solo cluster sulla base della distanza di y dal centroide del cluster. ‚Ä¢ Aggiornare i valori dei centroidi vk come media dei dati y appartenenti al centroide k La procedura descritta precedentemente minimizza la funzione obiettivo, ma non √® ovviamente un procedimento esaustivo. √à possibile quindi provare che l‚Äôalgoritmo k-means converge sempre, ma non √® detto che la configurazione di convergenza sia quella che trova un minimo assoluto della funzione obiettivo. L‚Äôalgoritmo k-means √® quindi un ottimizzatore locale. Poich√© lo stato di convergenza dell‚Äôalgoritmo dipende dalla definizione iniziale dei centroidi, √® possibile ripetere il procedimento pi√π volte assegnando in modo casuale il valore iniziale dei centroidi e tra le varie configurazioni stabili trovare quella dove la funzione obiettivo √® minima. Si noti che dal punto di vista dell‚Äôalgoritmo di clustering il numero di dimensioni che caratterizza i dati y e quindi i centroidi v √® irrilevante. In generale y sar√† un vettore a n dimensioni e v avr√† n dimensioni come y. La distanza verr√† calcolata nello spazio n-dimensionale dei dati y. In MATLAB l‚Äôalgoritmo K-MEANS √® implementato nella funzione kmeans (Statistics Toolbox). L‚Äôalgoritmo presenta alcune difficolt√†: ‚Ä¢ √à necessario un algoritmo per inizializzare i centroidi. Un possibile metodo √® far coincidere i C centroidi iniziali con C campioni scelti a caso dai dati. Oppure √® possibile definire i valori iniziali dei centroidi sulla base della conoscenza a priori dei dati (opzione start di kmeans) ‚Ä¢ Il risultato finale dipende dai centroidi iniziali. Questo pu√≤ essere sfruttato facendo girare pi√π volte l‚Äôalgoritmo in dipendenza da condizioni iniziali diverse e prendendo come risultato ottimo quello che minimizza J. (opzione nreplicates di kmeans). ‚Ä¢ Pu√≤ accadere che il un cluster si svuoti, per cui non pu√≤ essere aggiornato. In questo caso l‚Äôalgoritmo si blocca (opzione EmptyAction). Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini ‚Ä¢ Il risultato dipende dalla metrica (opzione Distance). A volte √® opportuno normalizzare le variabili rispetto alla deviazione standard (variabili standardizzate). ‚Ä¢ Il risultato dipende dal numero C di cluster che √® predefinito. Riguardo l‚Äôultimo punto, in generale non esiste un metodo per la stima del numero ottimo di cluster. Si utilizza un modello (ad esempio nella segmentazione di immagini possiamo conoscere a priori quanti tessuti vogliamo segmentare), oppure si fanno diversi tentativi con C diversi e si cerca di stimare la soluzione migliore utilizzando il valore di J. Una variante del k-means √® il metodo ‚ÄúIterative Self-Organizing Data Analysis Technique‚Äù (ISODATA), che √® sostanzialmente un algoritmo k-means in cui il numero di cluster non √® un parametro di input non modificabile ma uno dei risultati dell‚Äôalgoritmo che viene computato insieme ai cluster. Nell‚Äôapproccio ISODATA due cluster possono essere combinati tra loro se la loro separazione √® minore di una certa soglia mentre un singolo cluster pu√≤ essere diviso in due se √® troppo ‚Äúdisperso‚Äù nello spazio di ricerca. Quando applicato alla segmentazione di immagini, la metrica utilizzata √® tipicamente la differenza assoluta o la media quadratica delle differenze dei livelli di grigio. Nel caso di immagini singole le due metriche sono chiaramente coincidenti. Nel caso di sequenze temporali, si possono usare metriche di tipo correlativo che descrivono ad esempio la correlazione temporale tra la variazione del livello di segnale dei pixel nel tempo. Nel caso dell'immagine del fantoccio introdotta in precedenza, l‚Äôanalisi visiva dell‚Äôistogramma ci suggerisce l‚Äôuso di C=3 cluster. Il problema √® chiaramente monodimensionale (singola immagine). L'algoritmo kmeans con tre cluster ci fornisce tre centroidi (12.4, 174, 921) che rappresentano i valori tipici dei tre tessuti e le maschere dei tre tessuti sull'immagine:
![[Pasted image 20251129202948.png]]
Questa rappresentazione √® alternativa rispetto alle tre maschere distinte che avevamo introdotto in precedenza ma ha l‚Äôidentico significato. Il risultato √® molto simile a quello ottenuto con la valutazione manuale delle soglie, con la differenza che qui le soglie stesse vengono trovate automaticamente. Questo tipo di maschere possono essere visualizzate a falsi colori dove ad ogni colore corrisponde un tessuto come nella rappresentazione di destra. Supponiamo ora di acquisire lo stesso fantoccio con due sequenze MR diverse, come in figura. Il livello di grigio associato ai tre tessuti sar√† diverso tra le due immagini. Ogni dato sar√† quindi caratterizzato da una coppia di valori. Il funzionamento dell‚Äôalgoritmo di clustering √® esattamente lo stesso con la differenza che ora la distanza tra due pixel non √® pi√π la differenza assoluta tra due valori ma la distanza (euclidea o di Manhattan) tra i due pixel in uno spazio bi-dimensionale
![[Pasted image 20251129203004.png]]
in questo caso i dati da fornire all'algoritmo kmeans saranno un vettore Nx2, dove N √® il numero di pixel dell'immagine. L'algoritmo k-means ci restituir√† il valore dei centroidi, che sar√† rappresentato come un vettore 3x2, perch√© ogni centroide √® definito in uno spazio bidimensionale, ed una maschera come in precedenza.
![[Pasted image 20251129203020.png]]
Il metodo pu√≤ essere esteso a qualunque numero di immagini. √à importante notare come applicare un clustering multidimensionale abbia senso solo se si hanno pi√π misure su una stessa locazione spaziale. Le immagini 3D (in cui ogni voxel corrisponde ad una diversa locazione spaziale) vengono elaborate con un algoritmo di clustering monodimensionale esattamente come le immagini 2D, quindi come una lista di pixels (o voxel).
## Clustering non esclusivo (FCM)
Nel clustering non esclusivo, spesso definito fuzzy clustering, un dato pu√≤ appartenere a pi√π cluster con diversi livelli di appartenenza. La somma dei livelli di appartenenza su tutti i possibili cluster per un dato dovr√† essere 1. L‚Äôalgoritmo fuzzy c-means (FCM) quindi generalizza l‚Äôalgoritmo K-means, consentendo una segmentazione pi√π graduale basata sulla teoria del set di regole fuzzy. La fuzzy logic o logica sfumata √® un'estensione della logica booleana, basata su un grado di verit√† di ciascuna proposizione. √à fortemente legata alla teoria degli insiemi sfocati e, dopo essere gi√† stata intuita da pensatori precedenti, venne concretizzata da Lotfi Zadeh. La teoria degli insiemi fuzzy costituisce un'estensione della teoria classica degli insiemi poich√© per essa non valgono i principi aristotelici di non contraddizione e del terzo escluso (o del Tertium non datur). Il principio di non contraddizione stabilisce che, dati due insiemi A e !A (non-A), ogni elemento appartenente all'insieme A non pu√≤ contemporaneamente appartenere anche a non-A; l‚Äôintersezione di A e !A √® l‚Äôinsieme vuoto. Secondo il principio del terzo escluso, se un qualunque elemento non appartiene all'insieme A, esso necessariamente deve appartenere al suo complemento non-A. L'unione di un insieme A e del suo complemento non-A costituisce il dominio completo di definizione degli elementi di A. La modifica introdotta dalla logica Fuzzy √® di rifiutare questo assunto. Quando parliamo di grado di verit√† o valore di appartenenza intenderemo che una propriet√† pu√≤ assumere oltre che i valori vera (valore 1) o falsa (valore 0) come nella logica classica, anche valori intermedi. In logica fuzzy si pu√≤ ad esempio dire che un bambino appena nato √® giovane di valore 1, un diciottenne √® giovane 0,8, ed un sessantacinquenne √® giovane di valore 0,15 (tale valore dipende dall‚Äôet√† del docente che tiene il corso). Solitamente il valore di appartenenza si indica con u. √à importante notare che il concetto di appartenenza fuzzy non ha nulla a che vedere con il concetto di probabilit√†. Nella probabilit√† una affermazione o √® vera o √® falsa con una certa probabilit√†, mentre nella logica fuzzy √® insieme vera e falsa. L‚Äôalgoritmo fuzzy c-means (FCM) ha quindi la particolarit√† di consentire ad un dato di appartenere a pi√π cluster contemporaneamente. Il metodo √® stato ideato da Dunn nel 1973 e migliorato da Bezdek nel 1981). Il metodo √® basato sulla minimizzazione della funzione obiettivo:
![[Pasted image 20251129203057.png]]
dove m √® un numero strettamente maggiore di 1, ujk √® il grado di appartenenza di yj rispetto al cluster k, yj √® il j-esimo di N dati d-dimensionali appartenenti all‚Äôinsieme ‚Ñ¶, vk √® il centro d-dimensionale del cluster k, e || . || √® una distanza. C √® il numero di cluster. m √® un parametro detto fuzzyness dell‚Äôalgoritmo. Di solito si pone m=2. Si noti che se u √® una matrice binaria, cio√® pu√≤ assumere solo i valori 0 e 1, la funzione J diviene uguale a quella definita per il K-means, che si pu√≤ quindi considerare un caso particolare dell‚Äôalgoritmo FCM. In particolare, come si vede dalla figura al crescere del valore di m il peso dei valori di appartenenza ‚Äúpiccoli‚Äù nel computo di J diviene sempre minore, fino ad annullarsi per valori di m molto grandi. L‚Äôalgoritmo FCM va quindi a coincidere con il k-means per m che tende ad infinito. La complessit√† computazionale del problema √® equivalente a quella del K-means, per cui anche il partizionamento fuzzy √® ottenuto attraverso una ottimizzazione iterativa della funzione obiettivo, in modo simile a quanto visto per il K-means, attraverso l‚Äôaggiornamento della funzione di appartenenza uik e dei centri dei cluster vk:
![[Pasted image 20251129203130.png]]
Il processo iterativo si ferma quando la differenza tra il valore corrente di u ed il valore precedente √® pi√π piccola di una soglia. La procedura descritta converge ad un minimo locale della funzione obiettivo, JFCM. Si noti che a differenza del k-means dove la condizione di convergenza √® univoca nel FCM la condizione di convergenza dipende dalla soglia utilizzata. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 m=1,2,5,10,100 Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini L‚Äôalgoritmo FCM presenta gli stessi problemi associati all‚Äôalgoritmo k-means, con la possibile eccezione dello svuotamento dei cluster che √® meno probabile per la natura continua (e non discreta come nel k-means) del processo di ottimizzazione. L‚Äôalgoritmo di fuzzy clustering √® implementato in matlab attraverso la funzione fcm (Fuzzy Logic Toolbox).
Otterremo invece tre maschere, una per ogni tessuto, dove i valori delle maschere non sono pi√π binari ma sono distribuiti tra 0 e 1. Un valore 1 (bianco) indica la massima appartenenza del tessuto al cluster.
![[Pasted image 20251129203213.png]]
L‚Äôapproccio FCM √® di interesse nella segmentazione delle immagini mediche in quanto consente di tener conto del PVE. Infatti in una immagine biomedica il segnale di alcuni pixel/voxel sar√† dato da due o pi√π tessuti che si compenetrano nella stessa regione elementare di spazio nella quale viene misurato il segnale. L‚Äôapproccio FCM consente di assegnare a tali pixel/voxel un valore di appartenenza distribuito tra i tessuti presenti, interpretando nel modo corretto il PVE. A titolo di esempio, consideriamo l‚Äôimmagine in figura (imageNS) formata da sei pattern omogenei, di valore [20, 70, 150, 300, 550, 750], con l‚Äôaggiunta di rumore gaussiano e l‚Äôapplicazione di un filtro a media mobile per simulare il PVE.
![[Pasted image 20251129203228.png]]
Abbiamo evidentemente C = 6. Applichiamo all‚Äôimmagini il FCM attraverso la funzione MATLAB fcm. [center,U,J] = fcm(imageNS(:),6) Si noti che abbiamo trasformato l‚Äôimmagine in un vettore monodimensionale attraverso l‚Äôoperatore (:). La funzione fcm restituisce il valore dei centroidi (center), la funzione di appartenenza (U) e il valore J della funzione obiettivo durante le iterazioni. Il grafico della funzione J mostra come l‚Äôalgoritmo minimizzi il valore di J e si fermi quando J non varia pi√π in modo significativo.
![[Pasted image 20251129203240.png]]
I valori dei centroidi risultanti sono [19.95, 69.78, 153.42, 300.54, 548.46, 748.53] e rappresentano una stima del valore di segnale dei 6 pattern omogenei. U √® un array 6xN dove N √® il numero di pixel dell‚Äôimmagine. Ognuna delle 6 componenti di U rappresenta la mappa di appartenenza per un certo cluster. La mappa di appartenenza del cluster di 50 100 150 200 250 300 350 400 450 500 50 100 150 200 250 300 350 400 450 500 0 5 10 15 20 25 30 35 0 5 10 15 x 108 Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini valore massimo (750) risulta come in figura a sinistra, con un valore massimo (1) in corrispondenza del pattern.
![[Pasted image 20251129203254.png]]
A destra viene riportata la mappa di appartenenza del cluster con valore 550. Come si vede i pixel di bordo del pattern che assumono un valore intermedio tra 550 e lo sfondo hanno un grado di appartenenza minore di 1 (intorno a 0.5), tranne nell‚Äôinterfaccia tra il pattern ‚Äú550‚Äù e il pattern ‚Äú300‚Äù dove l‚Äôappartenenza assume un valore pi√π alto in quanto il segnale risultante dal PVE √® pi√π vicino al valore di segnale del pattern. Notiamo infine che ai pixel di bordo del pattern ‚Äú750‚Äù viene assegnato un grado di appartenenza elevato al cluster ‚Äú550‚Äù. Tali pixel infatti risultano dal PVE tra il pattern ‚Äú750‚Äù e il pattern ‚Äú70‚Äù e quindi possono assumere valori simili al pattern ‚Äú550‚Äù. Questa errata attribuzione √® tipica degli algoritmi di segmentazione a soglia quali l‚Äôalgoritmo FCM, che non avendo connotazioni topologiche non √® in grado di distinguere strutture topologicamente connesse o meno.
## Algoritmo EM (Expectation Maximization)
Gli algoritmi di segmentazione prima visti non assumono alcuna ipotesi sulla distribuzione probabilistica dei dati. Questo in generale pu√≤ non essere corretto. Consideriamo ad esempio la separazione tra aria e acqua in un fantoccio MR esaminandone l'istogramma:
![[Pasted image 20251129203320.png]]
Un algoritmo di clustering (o il metodo di Otsu) trover√† una soglia che massimizza la separazione tra i due picchi. Questa soglia per√≤ non terra conto del fatto che a causa dell'effetto volume parziale i pixel dell'acqua possono assumere comunque livelli di grigio inferiori alla soglia sui bordi. Inoltre la distribuzione dell'aria √® non gaussiana (√® di Rician) e quindi i pixel dell'aria hanno maggior probabilit√† di superare la soglia rispetto a quelli dell'acqua. L'approccio ti tipo EM consente di tener conto di tali peculiarit√† introducendo informazioni sulla distribuzione di probabilit√† dei vari cluster. L‚Äôapproccio di tipo EM introduce quindi il clustering basato su modelli (model-based approach), dove la distribuzione dei dati sui singoli cluster viene modellata attraverso funzioni probabilistiche note, tra le quali la distribuzione di tipo gaussiano √® la pi√π utilizzata. L‚Äôapproccio modellistico consente di tenere in conto eventuali ipotesi sulla generazione dei dati e sul rumore che li accompagna. In pratica, ogni cluster √® rappresentato matematicamente da una distribuzione di tipo parametrico, come le distribuzioni Gaussiana (Continua) o di Poisson (Discreta). L‚Äôinsieme dei dati √® modellato come una combinazione di queste distribuzioni. Le singole distribuzioni sono chiamate distribuzioni componenti. 0 50 100 150 200 250 300 0 1000 2000 3000 4000 5000 6000 HO Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini Il caso pi√π semplice nel clustering probabilistico √® quello della combinazione di gaussiane (mixture of Gaussian). I cluster sono modellati come gaussiane centrate sui centroidi dei cluster. In figura i cerchi in grigio rappresentano la varianza delle distribuzioni. Possiamo pensare che i due assi rappresentino il livello di grigio dei pixel di due immagini ed i punti nel piano il livello di grigio del singolo pixel. Vogliamo raggruppare gli spot in due classi, supponiamo quindi di avere due cluster (k=2). Dato un punto del piano, il punto avr√† una certa probabilit√† di essere stato generato da ognuno dei due cluster, probabilit√† che dipende dai parametri della gaussiana che descrive il cluster stesso. In particolare, una gaussiana con un centro ‚Äúlontano‚Äù dal punto o con una varianza piccola avr√† basse probabilit√† di aver generato il punto, e viceversa. Nel caso delle immagini, una immagine con basso SNR avr√† pi√π possibilit√† di generare un pixel lontano dal suo valor medio e viceversa.
Se i parametri che descrivono le gaussiane sono noti, il problema si riduce ad utilizzare come distanza nell‚Äôalgoritmo di clustering la probabilit√† che un certo dato sia stato generato da una certa gaussiana. Avremo quindi:
![[Pasted image 20251129203345.png]]
Il problema diviene pi√π complesso quando i parametri caratterizzanti le gaussiane non sono noti. L‚Äôalgoritmo di clustering dovr√† quindi stimare oltre ai cluster anche i parametri delle distribuzioni componenti. Quello che vogliamo trovare sono quindi i parametri delle gaussiane che hanno la maggiore probabilit√† di aver generato i dati osservati. Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini Il procedimento da utilizzare, detto EM (expectation maximization) √® di tipo iterativo. L‚Äôalgoritmo EM √® molto generale e pu√≤ essere utilizzato per le pi√π diverse distribuzioni di probabilit√†. Facciamo un esempio semplice nell‚Äôipotesi che le distribuzioni di probabilit√† siano Gaussiane. Il problema si pu√≤ formalizzare come segue: Siano date K classi; nel caso della segmentazione le classi corrispondono ai pattern dell‚Äôimmagine. Ogni classe abbia una probabilit√† ‚Äúa priori‚Äù Pk; nel caso della segmentazione la probabilit√† a priori corrisponde al numero normalizzato dei pixel appartenenti alla classe (altezza del picco nell‚Äôistogramma). Ogni classe abbia media Mk e SD Vk; nel caso della segmentazione media e SD descrivono la posizione e larghezza del picco dell‚Äôistogramma relativo alla classe. K (numero delle classi) √® noto mentre gli altri parametri devono essere determinati. Se ciascuna classe obbedisce ad una legge di probabilit√† gaussiana si pu√≤ definire per la distribuzione dei livelli di grigio dell‚Äôimmagine (cio√® l‚Äôistogramma dell‚Äôimmagine):
![[Pasted image 20251129203403.png]]
Si tratta di scegliere il vettore delle probabilit√† a priori P, delle medie M e delle SD œÉ in modo da minimizzare la differenza tra istogramma e somma di gaussiane. Il problema pu√≤ essere affrontato tramite l‚Äôalgoritmo EM che √® composto da due passi che vengono iterati. Nel caso della gaussian mixture, l‚Äôalgoritmo EM √® abbastanza semplice ed √® composto dai due passi seguenti: Si definisce un valore iniziale dei valori P, M e œÉ. Essendo l‚Äôalgoritmo EM un ottimizzatore locale il risultato finale dipender√† dai valori iniziali. Dati P, M e œÉ si calcolano le probabilit√† che un certo valore di segnale sia stato generato da un certa gaussiana : Passo E-step (expectation) calcolo della probabilit√† che il dato xj sia stato generato dalla gaussiana i.
![[Pasted image 20251129203418.png]]
che √® facilmente ricavabile dalla formula della gaussiana, essendo il valore della gaussiana stessa nel punto per la probabilit√† a priori che un pixel appartenga a quella gaussiana. Il secondo passo √® detto M-step (maximization) e consiste nell‚Äôaggiornare i parametri delle gaussiane in base al passo precedente. Definiamo:
![[Pasted image 20251129203432.png]]
In questo modo abbiamo definito delle nuove gaussiane e torniamo al passo E. L‚Äôalgoritmo al solito si ferma quando i valori della media, della varianza e dei pesi si stabilizzano. A questo punto i parametri trovati descrivono i cluster. E‚Äô possibile dimostrare che l‚Äôalgoritmo EM aumenta il grado di verosimiglianza della combinazione di gaussiane ad ogni iterazione e converge sotto certe condizioni ad un massimo locale della verosimiglianza della distribuzione di gaussiane. La dimostrazione √® estremamente complessa e non viene qui riportata. L‚Äôalgoritmo EM pu√≤ essere utilizzato non sono nella risoluzione del problema della gaussian mixture, ma in molte altre applicazioni. I problemi fondamentali in cui pu√≤ incorrere l‚Äôalgoritmo EM applicato alle gaussian mixture sono il fatto che due gaussiane convergano alla stessa gaussiana, con la scomparsa di un cluster, e che una gaussiana assuma varianza infinita e quindi si trasformi in un valore costante. Al solito variare le condizioni iniziali partendo da una stima ragionevole dei parametri pu√≤ ottimizzare il funzionamento dell‚Äôalgoritmo EM. L'algoritmo EM-GM √® implementato in Matlab dalla funzione fitgmdist nello Statistics and Machine Learning Toolbox. 
![[Pasted image 20251129203454.png]]
Applicato all'immagine del fantoccio MRI, il metodo fornisce tre gaussiane, con valori medi 11.7349 170.7140 905.0376 simili ai centroidi dei cluster prima trovati, e varianze 137, 858, 12000, probabilmente per l'effetto del volume parziale tra acqua e olio che costringe a ‚Äúspalmare‚Äù le due gaussiane. La somma delle gaussiane trovate rappresenta poi un approssimazione dell'istogramma dell'immagine, per cui l'algoritmo EM pu√≤ anche essere considerato un metodo per modellare l'istogramma.

## Clustering Gerarchico
In generale, √® possibile che un cluster di dati possa essere diviso a sua volta in sub-cluster pi√π piccoli, che a loro volta possono essere divisi in sub-cluster ancora pi√π piccoli e cos√¨ via. Un esempio tipico √® la classificazione degli organismi vegetali o animali, che vengono divisi in categorie sempre pi√π specifiche (ordini, specie). Nell‚Äôimaging il clustering gerarchico pu√≤ essere utilizzato per raggruppare immagini appartenenti ad esempio ad una serie temporale, come si vedr√† nel capitolo dedicato agli algoritmi di registrazione. Il clustering gerarchico √® una tecnica che permette di creare un albero gerarchico, in cui gli elementi dell‚Äôinsieme su cui si opera il clustering sono le foglie dell‚Äôalbero. Una riga orizzontale nell‚Äôalbero individua una serie di cluster analoghi a quelli ottenuti nel clustering tradizionale.  
Dato un insieme di N oggetti da raggruppare, ed una matrice NxN di distanze tra gli oggetti, il processo di clustering gerarchico pu√≤ essere definito come (S.C. Johnson 1967): 1. Si definiscono N cluster, uno per ogni oggetto. Abbiamo quindi N cluster che contengono ognuno un solo oggetto. Le distanze tra i cluster saranno uguali alle distanze tra gli oggetti che in questo primo passo si identificano con i cluster stessi. 2. Si identificano i due cluster pi√π vicini nel senso della distanza adottata, cio√® pi√π simili. Questi due cluster vengono raggruppati in un cluster unico, abbiamo cos√¨ N-1 cluster, uno con due oggetti e gli altri con un solo oggetto. 3. Ricomputiamo la matrice delle distanze, che sar√† ora una matrice N-1xN-1. 4. Si ripetono i passi 2 e 3 fino a quando non rimane un solo cluster che contiene N oggetti. Il passo 3 pu√≤ essere eseguito in modi diversi, in base ai diversi approcci possibili riconosciamo tre categorie di clustering gerarchico: single-linkage (singolo collegamento), complete-linkage (collegamento completo) e average-linkage (collegamento mediato). Nel single-linkage clustering, la distanza tra due cluster √® definita come la minima distanza tra tutti gli elementi di un cluster e tutti quelli dell‚Äôaltro cluster. In pratica si calcola la matrice delle distanze tra gli elementi dei due cluster e si prende come distanza tra i due cluster il minimo sulla matrice. Se invece di una funzione distanza si utilizza una funzione di similarit√†, cio√® una funzione che √® grande quando i due oggetti sono simili, si considerer√† il massimo della matrice di similarit√†. Nel complete-linkage clustering (chiamato anche metodo del diametro o del massimo), la distanza tra due cluster sar√† definita come il massimo sulla matrice delle distanze computata tra I dati sui due cluster. Nell‚Äôaverage-linkage clustering, la distanza tra due cluster sar√† la media della matrice delle distanze computata sui dati appartenenti ai due cluster. Una variazione abbastanza usata del metodo precedente √® il metodo che usa la mediana della matrice delle distanze invece che la media, che √® meno sensibile a dati spuri. L‚Äôapproccio descritto finora √® di tipo agglomerativo, perche si basa sul ragruppamento progressivo di cluster sempre pi√π grandi. Esiste anche un approccio inverso, dove si parte da un cluster che comprende tutti i dati e si procede per divisioni successive (divisive hierarchical clustering). Questo approccio √® comunque molto meno diffuso.
### Esempio di Single-Linkage Clustering
Consideriamo un approccio single-linkage e descriviamo come √® impostato un algoritmo che lo implementi. L‚Äôidea di base √® cancellare in modo progressivo righe e colonne della matrice delle distanze dei dati coinvolti nell‚Äôoperazione conglobando coppie di righe e di colonne in una sola riga o colonna. Sia D = [d(i,j)] la matrice NxN delle distanze. Vogliamo ottenere n cluster 0,1,......, (n-1) dove L(k) √® il livello gerarchico del singolo cluster. Il cluster m sar√† indicato con (m) mentre la distanza tra due cluster (r) e (s) sara indicata da d [(r),(s)]. L‚Äôalgoritmo proceder√† nel modo seguente: 1. Inizia con il cluster di livello L(0) = 0 e di posizione nella sequenza dei cluster m = 0. 2. Trova i due cluster pi√π simili, siano essi (r) e (s) d[(r),(s)] = min d[(i),(j)] dove l‚Äôoperazione di minimo √® estesa a tutte le possibili coppie di cluster. 3. Incrementa l‚Äôindice della sequenza dei cluster: m = m +1. Combina i cluster (r) and (s) in un singolo cluster di livello L(m) = d[(r),(s)] 4. Aggiorna la matrice delle distanze D, cancellando le righe e le colonne corrispondenti ai cluster (r) e (s) e aggiungendo una nuova riga ed una nuova colonna corrispondenti al nuovo cluster ottenuto al passo precedente. La distanza tra il nuovo cluster e un vecchi cluster k √® definita come: d[(k), (r,s)] = min d[(k),(r)], d[(k),(s)] 5. Se esiste un solo cluster, la procedura si ferma. Altrimenti vai al passo 2.
E‚Äô importante notare che il processo √® dipendente dalla funzione distanza scelta. Scelte diverse della funzione distanza producono risultati anche completamente diversi.
## Metriche negli algoritmi di clustering
Negli algoritmi d clustering √® importante la scelta della metrica, cio√® della grandezza che misura la differenza (o distanza) tra due punti nello spazio da classificare. La scelta pi√π immediata √® quella di distanze di tipo geometrico, scelte cio√® nella famiglia delle metriche di Minkowski:
![[Pasted image 20251129203717.png]]
che comprendono la classica distanza euclidea come caso particolare per p=2. Per p=1 abbiamo la cosiddetta distanza Manhattan. La tabella mostra le principali metriche utilizzate negli algoritmi di clustering come misura di distanza tra gli elementi. Come si vede la maggior parte delle distanze sono derivate dalla distanza di Minkowski. Un esempio di applicazione della correlazione come distanza pu√≤ essere la segmentazione di immagini 2D+T con iniezione di mezzo di contrasto, nella quale ci interessa raggruppare tra loro i pixel che rispondono in modo simile all‚Äôall‚Äôarrivo del contrasto. Infine √® opportuno notare come il funzionamento di un algoritmo di clustering multidimensionale dipenda dalla scala su cui sono misurati i valori dei dati su cui fare il clustering. Se i dati non sono omogenei, i dati con valori maggiori peseranno di pi√π nel computo della distanza falsando il comportamento dell‚Äôalgoritmo. Si pensi ad esempio ad un clustering in cui si utilizzano immagini su Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini 8 bit ed immagini a 16 bit. In questi casi √® opportuno normalizzare i dati su distribuzioni di tipo standard (tipicamente distribuzioni gaussiane con media zero e deviazione standard 1), un processo detto standardizzazione delle variabili. L‚Äôoperazione di standardizzazione in Matlab √® implementato dalle funzioni zscore o normalize (dalla versione R2018).
![[Pasted image 20251129203740.png]]

## Algoritmi di Labeling
Come detto in precedenza, la segmentazione a soglia non √® in grado di distinguere oggetti topologicamente diversi ma ai quali √® associato lo stesso livello di segnale. Nell‚Äôimmagine del fantoccio ad esempio abbiamo pattern topologicamente distinti che vengono estratti come un oggetto unico. Questo √® un limite intrinseco della segmentazione a soglia, che pu√≤ essere superato elaborando i dati della segmentazione stessa. Il metodo pi√π diretto √® scomporre la segmentazione usando un algoritmo detto ‚Äúlabel region‚Äù. L‚Äôalgoritmo lavora su una immagine binaria (quindi una maschera) e funziona nel seguente modo: Per ogni pixel dell‚Äôimmagine: 1) Se √® il primo pixel, crea un gruppo (blob) e aggiungi il pixel al blob 2) Per tutti i pixel dell‚Äôintorno: a. Controlla se sono gi√† stati assegnati a un blob. Se si, vai a 2 b. Controlla se appartengono allo stesso blob del pixel di partenza. Se si, aggiungili al blob e vai a 2 Completato il primo blob, si prende un pixel non appartenente al blob e si ritorna a 1), creando un nuovo blob. Si itera fino a quando tutti i pixel sono stati assegnati. I blob ottenuti possono essere classificati per grandezza e i blob pi√π piccoli eventualmente eliminati (come nel filtro mediano). Per pixel dell'intorno si intendono i pixel contenuti in un kernel centrato sul pixel stesso di dimensioni 3x3 e pu√≤ includere o meno le diagonali. In MATLAB la funzione da utilizzare √® bwlabel o bwconncomp (Vedi esempio ES-MAT-5). Se consideriamo il fantoccio MR (sinistra) e applichiamo una segmentazione a soglia con ad esempio l‚Äôalgoritmo kmeans, ricaveremo una maschera dell‚Äôacqua come al centro della figura. L‚Äôalgoritmo di labeling fornir√† una mappa in cui le regioni non appartenenti alla maschera dell‚Äôacqua saranno settate a 0, mentre la maschera verr√† divisa in una serie di regioni topologicamente connesse identificate da un indice univoco.
![[Pasted image 20251129203809.png]]
## Region Growing
Una estensione della segmentazione a soglia √® l'algoritmo ‚Äúregion growing‚Äù, dove partendo da un pixel all‚Äôinterno dell‚Äôoggetto da segmentare si estende la segmentazione a tutta la regione di interesse. In questo caso la soglia √® quindi definita in modo locale come la differenza di segnale tra un pixel ed i pixel vicini. Questo approccio sfrutta le informazioni spaziali e garantisce la formazione di regioni tra loro collegate. Di fatto √® una implementazione ricorsiva di una segmentazione effettuata per pixel adiacenti. La routine prevede la definizione di un punto di partenza all‚Äôinterno del pattern da riconoscere. Il valore di livello di grigio cos√¨ individuato costituisce il punto di partenza (detto ‚Äòseed pixel‚Äô, ‚Äòpixel seme‚Äô) per la successiva elaborazione: si vanno ad analizzare ricorsivamente i pixel adiacenti a quello selezionato inizialmente. Quelli che hanno una differenza di livello di grigio appartenente ad un intervallo prefissato vengono selezionati, gli altri scartati. L‚Äôapproccio Region Growing prevede quindi i seguenti passi: 1. Si sceglie un arbitrario ‚Äòseed pixel‚Äô che viene confrontato con quelli adiacenti. 2. Dal seme si passa ad una regione che ‚Äòcresce‚Äô aggiungendo progressivamente quei pixel adiacenti che sono ‚Äòsimili‚Äô a quello iniziale secondo criteri di somiglianza fissati. 3. Quando la crescita della regione si ferma il processo termina Il seed pixel pu√≤ essere scelto in modo manuale o con algoritmi automatici. La funzione ‚Äúpaint‚Äù definita in tutti i programmi di image processing utilizza questo metodo. Questo tipo di segmentazione consente di individuare in modo ottimale i confini degli oggetti individuati per osservazione. diff di questi a met√†fra algoritmo disegni e di labeling lavora su lived digrigio Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini Il punto cruciale della tecnica ‚Äúregion growing‚Äù √® il criterio di similarit√† dei pixel, che determina se un pixel debba o meno essere aggiunto alla regione. Nel caso di immagini binarie, il criterio di simiglianza si identifica con l'uguaglianza del valore dei pixel. In questo caso l‚Äôalgoritmo si comporta come l‚Äôalgoritmo di labeling. Nel caso di immagini a livello di grigio, la condizione di inclusione diviene ABS(s0-s1) < T, dove T √® un valore di soglia e ABS √® il valore assoluto. Si include quindi un pixel se il suo valore differisce da quello di un pixel adiacente appartenente alla regione per meno di T. Per il buon funzionamento dell'algoritmo, T deve essere abbastanza grande da includere tutti i pixel nella regione omogenea che vogliamo segmentare e abbastanza piccolo da impedire che la segmentazione ‚Äúdebordi‚Äù nei tessuti vicini.
![[Pasted image 20251129203837.png]]
In figura da sinistra a destra un esempio di segmentazione region growing con T ottimale, con T troppo basso (alcuni pixel vengono persi) e con T troppo alto. T pu√≤ essere definito esplicitamente come numero di livelli di grigio (es T=50). E' anche possibile definire T come percentuale rispetto al valore dei pixel, la condizione di inclusione diviene ABS(s0- s1)/MEAN(s0,s1) < T . In questo caso T sar√† compreso tra 0 e 1 e un pixel verr√† incluso se la sua differenza rispetto al pixel vicino √® inferiore al 100xT %. Un approccio ragionevole pu√≤ essere quello di definire T come proporzionale al rumore sull'immagine. Infatti, se consideriamo una zona omogenea dell'immagine, come ad esempio l'acqua o l'olio nel fantoccio, il segnale nella regione omogenea sar√† S0+N(0,œÉ) nel caso di rumore gaussiano a media nulla. Se ad esempio poniamo T=1.96œÉ, otterremo di includere il 95% dei pixel della regione. Questo approccio √® simile a quello visto nel design dei filtri adattivi. L'algoritmo di region growing √® implementato in MATLAB dalla funzione grayconnected.
## Segmentazione a contorni
Un approccio diverso al problema della segmentazione consiste nell'identificare sull'immagine i contorni dei pattern, che corrispondono alle zone di transizione tra due tessuti diversi. Consideriamo l'immagine del fantoccio in figura ed il relativo profilo dei livelli di grigio estratto dall'immagine stessa.
![[Pasted image 20251129203925.png]]
I contorni visti sul profilo saranno caratterizzati da una transizione netta del segnale. Se calcoliamo la derivata del profilo, valori alti della derivata corrisponderanno ai contorni, come si vede dalla figura. Per trovare i contorni, possiamo definite una soglia sul valore assoluto della derivata, valori della derivata superiori alla soglia definiscono una transizione e quindi un contorno.
![[Pasted image 20251129203940.png]]
Estendendo il concetto in 2D o 3D, un mappa di gradiente dell'immagine consente di identificare i contorni. La mappa di gradiente di una immagine pu√≤ essere ottenuta in vari modi, tipicamente attraverso un filtro convolutivo. Ad esempio il filtro di Sobel calcola la derivate del gradiente per righe e per colonne utilizzando i due kernel convolutivi:
la dimensione del kernel pu√≤ essere aumentata per ottimizzare il calcolo. Le operazioni di filtraggio convolutivo possono essere implementate attraverso la funzione conv2 di MATLAB, tuttavia √® pi√π opportuno utilizzare delle funzioni specifiche. In particolare la funzione edge dell'Image Processing Toolbox trova i contorni dell'immagine attraverso una serie di filtri, che possono essere scelti tra quello di Sobel, di Roberts, Lapalciano, etc. Sull'immagine di gradiente viene applicata una soglia per tagliare i valori bassi di gradiente e evidenziare i contorni.
![[Pasted image 20251129204019.png]]
Una volta ottenuta la mappa dei contorni (edge map) con questo tipo di algoritmi, √® necessario costruire un contorno continuo che unisca le aree ad alto valore di gradiente. Uno degli approcci pi√π 50 100 150 200 250 50 100 150 200 250 Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini semplici per collegare i punti di discontinuit√† in un‚Äôimmagine di gradiente √® quello di analizzare le caratteristiche dei pixel in un intorno ristretto (3x3 o 5x5) del punto in esame. In altre parole si cercano delle propriet√† comuni ai punti dell‚Äôintorno e successivamente si collegano i punti simili con qualche criterio. La creazione di un contorno continuo da una mappa di gradiente utilizza tipicamente due parametri (algoritmo di Canny): 
1) Intensit√† della mappa di gradiente. Un punto nell‚Äôintorno √® simile al punto considerato se la differenza del valore di modulo del gradiente nel punto √® minore di una soglia T![[Pasted image 20251129204054.png]]
2) Direzione del gradiente. Un punto nell‚Äôintorno √® simile al punto considerato se la direzione del gradiente nel punto (data dal rapporto tra le componenti x e y del gradiente) √® minor e di una soglia A ![[Pasted image 20251129204108.png]]
Il processo pu√≤ essere completato eliminando piccoli tratti di segmenti isolati e colmando piccoli intervalli tra i segmenti
## Active Contours (Snakes)
I metodi prima visti hanno il limite fondamentale di non includere al loro interno un modello dell‚Äôoggetto da segmentare. Nell‚Äôimaging biomedico, ad esempio, possiamo senz‚Äôaltro supporre che gli organi siano oggetti con una superficie che non presenta spigoli o punti ad elevata curvatura. Per molti organi abbiamo inoltre una informazione a priori sulla forma, ad esempio un vaso sar√† simile ad un cilindro, un ventricolo visto in asse lungo ad un ellissoide, etc. Esistono vari algoritmi che introducono queste informazioni ottimizzando la segmentazione. L‚Äôesempio pi√π noto sono gli algoritmi a contorni attivi o snakes. L‚Äôalgoritmo tradizionale, introdotto per la prima volta da Kass, consiste nell‚Äôinizializzare una certa curva (snake) e nel deformarla poi in modo da farla convergere in corrispondenza di un minimo locale di energia, cio√® di un contorno dell‚Äôimmagine. Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini Il primo problema da affrontare √® quindi quello della definizione del contorno iniziale: essendo un metodo di analisi locale infatti i risultati dipenderanno dal modo in cui √® stato inizializzato lo snake. Pertanto in dipendenza dell‚Äôapplicazione si dovr√† decidere se inizializzare automaticamente la curva oppure usare una procedura semiautomatica in cui le condizioni iniziali sono gestite direttamente da un utente esperto. La curva iniziale C √® definita da un insieme di N punti ordinati caratterizzati dalle loro coordinate (xi,yi) per i=1,..,N. La curva √® chiusa quindi il punto N √® connesso con il punto 1.
Possiamo anche definire C come una curva parametrica in s dove s indica la posizione del punto sulla curva:
![[Pasted image 20251129204138.png]]
![[Pasted image 20251129204151.png]]
La prima componente √® legata alla derivata prima e rappresenta la tendenza dello snake a mantenere la sua forma opponendosi alla trazione. L‚Äôenergia elastica √® legata alla distanza dei punti dello snake, aumentando la distanza tra due punti aumenta anche l‚Äôenergia elastica. La seconda componente (rigidit√†) √® legata alla derivata seconda e rappresenta la tendenza dello snake ad opporsi alle modifiche della sua curvatura. Impedisce allo snake di ‚Äúingarbugliarsi‚Äù. Manipolando la tensione o elasticit√† e la rigidit√† dello snake √® possibile modificare l‚Äôimportanza relativa delle due componenti dell‚Äôenergia interna. Intuitivamente si pu√≤ pensare allo snake come ad un elastico, che se lasciato libero riassume la sua forma originale grazie alla sua elasticit√†. Il secondo contributo viene di solito chiamato energia dell‚Äôimmagine o energia esterna in quanto viene ricavata dall‚Äôimmagine stessa in modo da assumere valore minimo nelle zone di maggiore interesse come ad esempio i bordi. Tipiche e semplici funzioni di energia dell‚Äôimmagine sono legate all'edge map dell‚Äôimmagine:
![[Pasted image 20251129204208.png]]
corrisponder√† ai contorni definiti sull‚Äôimmagine stessa a meno dei limiti di rigidit√† ed elasticit√† imposti. L‚Äôalgoritmo per minimizzare l‚Äôenergia √® un processo iterativo che parte da un contorno iniziale e converge (se la differenza della posizione dello snake tra due iterazioni √® abbastanza piccola) ad un minimo locale. Il processo iterativo √® efficiente dal punto di vista computazionale (ogni passo √® rappresentato dalla moltiplicazione di matrici numeriche) ed il suo risultato dipende dai valori D, E e N. Da un punto di vista pratico l‚Äôutilizzo del metodo AC (active contours) richiede il tuning accurato dei parametri D, E e N e la possibilit√† di definire un contorno iniziale abbastanza vicino al risultato desiderato in modo da ottimizzare la convergenza dell‚Äôoperatore locale. Come si osserva dall‚Äôequazione dello snake, le forze esterne sono computate solo sullo snake stesso. Questo implica che lo snake nella sua evoluzione ‚Äúvede‚Äù i contorni dell‚Äôimmagine solo quando gli attraversa. Una modifica dell‚Äôalgoritmo implica l‚Äôuso di informazioni interne ed esterne allo snake (region-based models) in modo da favorire la convergenza. L‚Äôimplementazione pi√π nota √® l‚Äôalgoritmo di Chan-Vese che troviamo implementato in MATLAB. Consideriamo una immagine con due pattern con livello di grigio I1 e I2, a meno del rumore e di altri fattori quali il PVE e l‚Äôattenuazione. Nell‚Äôapproccio di Chan-Vese l‚Äôenergia da minimizzare √® valutata come:
![[Pasted image 20251129204222.png]]
Dove I √® l‚Äôimmagine, C1 √® la media dei pixel dell‚Äôimmagine dentro il contorno C, C2 √® la media dell‚Äôimmagine fuori dal contorno C. Chiaramente F(C) si minimizza quando C corrisponde al bordo tra i due pattern e divide l‚Äôimmagine in due regioni omogenee in cui F1 e F2 sono uguali alla potenza del rumore associato all‚Äôimmagine. L‚Äôapproccio realmente implementato √® pi√π complesso in quando si estende ad una immagine ad N pattern omogenei. In MATLAB la funzione activecontour implementa le versioni proposte da Chan (Chan-Vese) e Caselles (edge). La funzione dei parametri alfa e beta dello snake √® svolta dai parametri ContractionBias (determina se il contorno si espande o si restringe, alfa) e SmoothFactor (determina la regolarit√† del contorno, beta). Gli algoritmi a contorni attivi funzionano su immagini bidimensionali. In teoria √® possibile estendere l‚Äôalgoritmo ad immagini 3D, in questo caso il contorno diviene una superficie chiusa (balloon) definita da una mesh geometrica come negli algoritmi di visualizzazione 3D. Nella pratica nell‚Äôelaborazione di immagini 3D si utilizza un approccio diverso detto level set.

## Level set (o implicit Active Contour)
L‚Äôalgoritmo level set (o implicit Active Contour) definisce il contorno C come l‚Äôintersezione di una funzione )(x,y) con il piano dell‚Äôimmagine che corrisponde a )(x,y)=0.
![[Pasted image 20251129204248.png]]
![[Pasted image 20251129204304.png]]
Partendo da un valore iniziale di ), e quindi di C, viene applicato un algoritmo iterativo che simula nel discreto l‚Äôevoluzione temporale ed il contorno si modifica seguendo l‚Äôevoluzione di ) fino alla convergenza. Una potenzialit√† interessante dell‚Äôalgoritmo level set √® che il contorno definito da ) si pu√≤ dividere in due o pi√π contorni durante la progressione dell‚Äôalgoritmo (e viceversa) permettendo una maggiore flessibilit√† rispetto all‚Äôalgoritmo a contorni attivi
## Watershed transform
Una importante famiglia di tecniche per la segmentazione di immagini si basa su concetto della ricerca delle componenti connesse (Connected component, CC). Il concetto alla base di queste tecniche √® la cosiddetta watershed transform, (spartiacque) nella quale il valore di livello di grigio dell‚Äôimmagine rappresenta la profondit√† di un pixel rispetto ad un valore di riferimento. Una immagine viene quindi vista come una struttura (ad esempio un lago) che viene via via riempita con acqua in una certa posizione. Man mano che si versa l‚Äôacqua, il pelo dell‚Äôacqua stessa si alza disegnando delle strutture (maschere) che individuano le regioni sotto una certa altezza. Quando si arriva ad un watershed, l‚Äôacqua trabocca in una regione vicina. L‚Äôalgoritmo, concettualmente simile al region growing, individua le configurazioni ‚Äúborderline‚Äù che precedono il travaso dell‚Äôacqua da una regione dove la superficie dell‚Äôacqua √® ‚Äústabile‚Äù ad un‚Äôaltra.
![[Pasted image 20251129204329.png]]
A Th=1 viene coperto lo zero-padding, a Th=20 una parte del fondo. Ad un certo punto la maschera si stabilizza fino a quando il livello non supera lo spartiacque della parete del fantoccio, dopodich√© ricomincia a crescere lentamente fino a stabilizzarsi di nuovo fino al riempimento dell‚Äôolio.
![[Pasted image 20251129204400.png]]
![[Pasted image 20251129204408.png]]


## Algoritmo MSERs
L‚Äôalgoritmo prima descritto pu√≤ essere generalizzato ottenendo l‚Äôalgoritmo Maximally Stable Extremal Regions (MSERs). L‚Äôalgoritmo MSERs ricerca le regioni estreme massimamente stabili all‚Äôinterno dell‚Äôimmagine. Le regioni estreme sono definite come le regioni connesse i cui livelli di grigio sono tutti al di sopra o al di sotto dei valori dei pixel che circondano la regione. Le connessioni tra un pixel ed i pixel circostanti come al solito possono essere definiti dai 4 (6) pixel vicini o dagli 8 (24) pixel vicini in 2D o 3D, rispettivamente. Come visto in precedenza, tra le regioni estreme quelle massimamente stabili sono quelle dove, definita una soglia Œî, la variazione del numero di pixel della regione variando il livello di grigio g da g-Œî a g+Œî √® un minimo locale. Il valore di Œî determina il numero di regioni che vengono trovate, che √® inversamente proporzionale a Œî. L‚Äôalgoritmo MSERs nella sua forma originaria prevede i seguenti passi: ‚Ä¢ I pixel dell‚Äôimmagine vengono ordinati secondo il valore del livello di grigio ‚Ä¢ Partendo dal valore pi√π basso di livello di grigio, i pixel vengono inseriti nell‚Äôimmagine (si considera di partire da una immagine vuota) e si individuano le componenti connesse con un opportuno algoritmo di labeling. ‚Ä¢ Viene memorizzata per ogni livello di grigio la lista delle componenti connesse e la loro area. Se due componenti si uniscono, la pi√π piccola viene inglobata nella pi√π grande. Si ottiene quindi una lista che per ogni livello di grigio contiene l‚Äôarea delle componenti connesse. Per ogni componente, si considera la curva area vs livello di grigio ed in corrispondenza dei minimi locali della curva si individuano le componenti massimamente stabili. Dal punto di vista computazionale, il punto critico √® il terzo. Bisogna per ogni passo ordinare le componenti per grandezza e riconoscere quali provengono da componenti esistenti al passo precedente. Per ottimizzare la procedura si memorizzano i dati con un approccio Component Tree, nel quale le componenti vengono memorizzate in un albero che consente di realizzare l‚Äôalgoritmo in modo efficiente dal punto di vista computazionale. L‚Äôalgoritmo √® implementato dalla funzione Matlab detectMSERFeatures. In figura √® illustrato il risultato dell‚Äôapplicazione dell‚Äôalgoritmo al fantoccio prima considerato.
![[Pasted image 20251129204429.png]]

## Component Tree
L‚Äôapproccio Component Tree come detto in precedenza permette di memorizzate il contenuto di una immagine in un albero, permettendo di realizzare algoritmi di elaborazione dell‚Äôimmagine in modo efficiente. Si tratta di un approccio molto usato e vale quindi la pena di esaminarlo in maggior dettaglio. Consideriamo una immagine a quattro livelli di grigio come quella in figura. L‚Äôimmagine contiene 8 o 9 pattern a seconda del tipo di connessione considerata, per un kernel di connessione a 8 elementi contiamo 8 pattern distinti. Al passo uno dell‚Äôalgoritmo viene selezionata la componente a minima intensit√† (1) c1, che va a costituire la radice dell‚Äôalbero e rappresenta il background. Al passo due, alla componente uno vengono aggiunte due componenti topologicamente distinte c2 e c3, con livello di grigio 2, che diventano due rami dell‚Äôalbero. Al passo tre vengono aggiunte tre componenti (c4, c5, c6), con livello di grigio 3, delle quali c6 √® contigua a c3 e quindi va nel ramo corrispondente dell‚Äôalbero, mentre le altre due nell‚Äôaltro essendo contigue a c2. Al passo quattro l‚Äôalbero si completa con le componenti c7 e c8 relative al livello gi grigio 4. In generale i livelli dell‚Äôalbero saranno pari al numero di livelli di grigio presenti nell‚Äôimmagine ed il numero di nodi sar√† uguale al numero di pattern presenti nell‚Äôimmagine.
![[Pasted image 20251129204455.png]]
In ogni nodo sono memorizzati i pixel appartenenti al nodo. Il peso delle connessioni (edge) tra nodi √® dato dalla differenza di livello di grigio tra i livelli (in questo caso uno per tutte le connessioni).
![[Pasted image 20251129204509.png]]

esempio se selezioniamo i due rami principali dell‚Äôalbero attraverso una funzione di ricerca depthfirst search, otteniamo i due ‚Äúmacro-pattern‚Äù in cui √® divisa l‚Äôimmagine, come illustrato in figura. Una segmentazione a soglia si ottiene immediatamente selezionando tutti i nodi sopra un certo livello. Il region-growing √® equivalente a partire da un nodo e muoversi sull‚Äôalbero con la condizione che il peso di una connessione deve essere minore uguale alla soglia di tolleranza dell‚Äôalgoritmo. In generale molti algoritmi di filtraggio e segmentazione possono essere implementati utilizzando in modo efficace la rappresentazione ad albero connesso.

## Skeletonization
![[Pasted image 20251129204552.png]]
Nell‚Äôelaborazione di immagini biomediche, una volta definita la maschera che definisce una certa struttura anatomica, pu√≤ essere utile caratterizzare la struttura da un punto di vista geometrico, estraendone i caratteri fondamentali, quali la connettivit√†, la topologia, la lunghezza, la direzione e l'ampiezza. Questo pu√≤ essere ottenuto attraverso l‚Äôestrazione del cosiddetto scheletro topologico (skeleton). Intuitivamente, lo scheletro topologico di una maschera si ottiene ‚Äúassottigliando‚Äù la maschera stessa fino ad ottenerne una versione essenziale, tipicamente composta da linee. In figura √® mostrato lo scheletro topologico dell‚Äôalbero bronchiale estratto da una maschera dell‚Äôalbero stesso ottenuta da immagini TAC. In questo caso lo scheletro estratto pu√≤ essere utile in varie applicazioni. Ad esempio lo scheletro, rappresentando la linea di equidistanza dalle pareti del bronco (center line) pu√≤ essere usata come guida per applicazioni di endoscopia virtuale. Dallo scheletro √® possibile individuare facilmente le biforcazioni dell‚Äôalbero bronchiale, permettendo la costruzione di un albero binario che permette di confrontare il paziente in esami eseguiti a tempi diversi o pazienti diversi tra loro. L‚Äôoperazione di skelethonization pu√≤ essere eseguita in vari modi. I due approcci fondamentali sono il thinning (assottigliamento) morfologico e il calcolo della distance transform dell‚Äôimmagine. L‚Äôoperazione di thinning si basa sull‚Äôidea di eliminare i pixel di confine della maschera in modo progressivo (erosione) riducendone le dimensioni fino a quando non √® pi√π possibile assottigliare ulteriormente la maschera e si ottiene lo scheletro topologico. Per formulare in termini algoritmici la funzione di thinning √® utile introdurre la hit-and-miss transform, (HAM) che, come avviene nei filtri spaziali, √® basata sulla convoluzione della maschera con un kernel (structuring element). Il kernel contiene la descrizione della struttura
geometrica che si vuole individuare. La struttura del kernel (supponendo di voler individuare un angolo della maschera) sar√† del tipo in figura, dove 0 indica un pixel appartenente allo sfondo della maschera, 1 un pixel appartenente alla maschera e le caselle bianche indicano locazioni non di interesse. In altri termini le caselle vuote vengono ignorate ed il loro scopo √® ottenere un kernel quadrato pi√π comodo da usare a fini computazionali. Analogamente alla convoluzione spaziale, il kernel viene fatto scorrere sulla maschera e se esiste una totale corrispondenza tra il kernel e la maschera sottostante il pixel della maschera risultante viene posto ad uno, altrimenti viene lasciato a zero. Tipicamente vengono generate una serie di versioni del kernel che corrispondono alle possibili orientazioni della struttura che interessa determinare, ad esempio nel caso in oggetto potremmo avere i quattro kernel:
![[Pasted image 20251129204542.png]]
Che corrispondono alla ricerca di un angolo nella maschera nelle quattro orientazioni possibili. L‚Äô OR logico delle quattro maschere risultanti permette di identificare gli angoli sulla maschera.
![[Pasted image 20251129204614.png]]
Un esempio di uso della hit-and-miss transform √® il rilevamento dei punti isolati di una maschera, che pu√≤ essere utile nella correzione degli errori di segmentazione. In questo caso il kernel da utilizzare risulta:
![[Pasted image 20251129204633.png]]
E si ottiene una maschera che definisce i punti isolati della maschera di input. La trasformazione HAM √® implementata in MATLAB dalla funzione bwhitmiss. Nel caso del thinning si pu√≤ usare una coppia di kernel del tipo:
![[Pasted image 20251129204646.png]]
L‚Äôapplicazione di questi due kernel equivale a conservare i pixel che sono al centro di un ottagono che √® totalmente incluso nella maschera. I kernel vengono ruotati nelle quattro direzioni possibili ottenendo quindi 8 trasformazioni hit-and-miss. L‚Äôoperazione di thinning si ottiene calcolando:
![[Pasted image 20251129204657.png]]
In pratica il risultato della trasformazione HAM viene sottratto alla maschera originale, che quindi si ‚Äùassottiglia‚Äù. Per ottenere lo skeleton l‚Äôoperazione di thinning viene iterata fino a quando la maschera originale non si modifica ulteriormente. Se consideriamo la maschera originale a sinistra l‚Äôapplicazione dell‚Äôoperazione di thinning porta alla maschera al centro, l‚Äôeffetto del thinning √® visibile nell‚Äôimmagine a destra che rappresenta la differenza tra le due maschere.
In MATLAB l‚Äôoperazione prima descritta di skelethonization √® implementata direttamente nella funzione bwmorph attraverso l‚Äôopzione 'skel'.
Il calcolo della distance transform dell‚Äôimmagine prevede invece che per ogni punto della maschera di input venga calcolata la distanza minima dal contorno della maschera stessa (o dallo sfondo della 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 50 100 150 200 250 300 Vincenzo Positano Modulo Elaborazione delle Bioimmagini ‚Äì corso Bioimmagini maschera). Di solito si utilizza la distanza City-block o `chessboard' che vale il numero di pixel che bisogna attraversare per andare da un pixel all‚Äôaltro dell‚Äôimmagine (nella City-block non si possono usare le diagonali). Si ottiene quindi una immagine di dimensioni uguali all‚Äôoriginale e a cui a ciascun pixel corrisponde un livello di grigio pari alla minima distanza dai bordi. I pixel con valore pi√π alto rappresentano lo skeleton in quando sono i pi√π ‚Äúcentrali‚Äù. Utilizzando una soglia si pu√≤ estrarre lo skeleton vero e proprio. In MATLAB la distance transform viene computata con la funzione bwdist che permette di definire il tipo di distanza da utilizzare. La funziona computa la distanza da un pixel non nullo, quindi per ottenere lo skeleton va invertita la maschera.

---

> [!nav]- Navigazione
> **Precedente:** [[1. Intro|‚Üê Introduzione Segmentazione]] | [[../INDICE|üìë Indice]]






